#+TITLE: 《深度学习》笔记
#+AUTHOR: zhufuge
#+DATE: <2018-01-16 周二 10:23>

* 第一章 引言
  历史及发展过程的介绍。

  抽象和形式化的任务对人类而言很难，对计算机而言很容易。


  一些 AI 项目力求将关于世界的知识用形式化的语言进行硬编码，即使用知识库，但都很
  难有进展。

  机器学习依赖于对数据的表示。许多人工智能任务需要先提取一个合适的特征集，然后将
  这些特征提供给简单的机器学习算法。但很多时候我们很难知道应该提取哪些特征。


  解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出，
  即 *表示学习* 。该方法学习到的表示往往比手动设计的表示表现得更好。

  设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差
  因素。这些因素通常是不能被直接观察到的量。例如，当分析汽车的图像时，变差因素包
  括汽车的位置，它的颜色，太阳的角度和亮度等。

  但从原始数据中提取如此高层次、抽象的特征是非常困难的。



  *深度学习* 通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。
  
  在书中的例子中，输入展示在 *可见层* ，然后是一系列从图像中提取越来越多抽象特征
  的 *隐藏层* 。模型必须确定哪些概念有利于解释观察数据中的关系。

  - 第一层通过比较相邻像素的亮度来识别边缘。
  - 第二层搜索可识别为角和扩展轮廓的边集合。
  - 第三层找到轮廓和角的特定集合来检测特定对象的整个部分。


  最后，识别图像中存在的对象。



  度量模型深度的方式：
  1. 基于评估架构所需执行的顺序指令的数目。流程图最长路径视为模型深度。
  2. 在深度概率模型中使用的方法，将描述概念彼此如何关联的图的深度视为模型深度。


  包含关系：深度学习）表示学习）机器学习）AI）

** 1.2 深度学习的历史趋势
  三次浪潮： *控制论* -> *联结主义* -> *深度学习*

  现代术语“深度学习”超越了目前机器学习模型的神经科学观点。它诉诸于学习多层次组
  合这一更普遍的原理。

  神经科学被视为深度学习研究的一个重要灵感来源，但它已不再是该领域的主要指导。仅
  通过计算单元之间的相互作用而变得智能的基本思想是受大脑启发的。



  联结主义的中心思想是，当网络将大量简单的计算单元连接在一起时可以实现智能行为。

  “大数据”时代使机器学习更加容易。一个粗略的经验法则是，监督深度学习算法在每类
  给定约 5000 个标注样本情况下一般将达到可以接受的性能，当至少有 1000 万个标注样
  本的数据集用于训练时，它将达到或超过人类表现。

  自从隐藏单元引入以来，人工神经网络的规模大约每 2.4 年扩大一倍。这种增长是由更
  大内存、更快的计算机和更大的可用数据集驱动的。



* 第二章 线性代数
** 2.1 概念
   - 标量（scalar）：一个标量就是一个单独的数。
   - 向量（vector）：一个向量就是一列数。
   - 矩阵（matrix）：矩阵就是一个二维数组。
   - 张量（tensor）：一般地，一个数组中的元素分布在若干维坐标的规则网络中，我们称
     之为张量。即表示 n 维数组。

  *线性组合* 简单的说，就是每行乘以一个数（可能为0）之后对应位置叠加。



** 2.5 范数
  有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为 *范数* （norm）
  的函数衡量向量大小。范数（包括 L^p  范数）是将向量映射到非负值的函数。直观上来
  说，向量 *x* 的范数衡量从原点到点 *x* 的距离。

  当 p = 2 时，L^2  范数被称为欧几里得范数（Euclidean norm）。它表示从原点
  出发到向量 x 确定的点的欧几里得距离。L^2  范数在机器学习中出现地十分频繁，经
  常简化表示为 ∥x∥，略去了下标 2。平方 L^2  范数也经常用来衡量向量的大小，可以
  简单地通过点积 x^T x 计算。



** 2.7 特征分解
  通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。

  方阵 *A* 的 *特征向量* 是指与 *A* 相乘后相当于对该向量进行缩放的非零向量 *v* ：

  #+BEGIN_CENTER
  Av = λv
  #+END_CENTER

  标量 λ 被称为这个特征向量对应的特征值。

  *A* 的 *特征分解* 为： =A = Vdiag(λ)V^-1=

  每个实对称矩阵都可以分解成实特征向量和实特征值： =A = QΛQ^T= 。其中 Q 是 A 的特
  征向量组成的正交矩阵，Λ 是对角矩阵。



** 2.8 奇异值分解
   每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。可以分解非方阵。

   矩阵 A 分解为三个矩阵的乘积：
   #+BEGIN_CENTER
   A = UDV^T 
   #+END_CENTER
   假设 A 是一个 m×n 的矩阵，那么 U 是一个 m×m 的矩阵，D 是一个 m×n 的矩阵，V
   是一个 n×n 矩阵。

   *Moore-Penrose 伪逆* 可以解决非方阵的求逆问题。



** 2.10 迹运算
   迹运算返回的是矩阵对角元素的和。

   有趣的是：
   #+BEGIN_CENTER
   Tr(ABC) = Tr(BCA) = Tr(CAB)
   #+END_CENTER
   即更一般地，即使循环置换后矩阵乘积得到的矩阵形状变了，迹运算的结果依然不变。



* 第三章 概率与信息论

  概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方法，也提供
  了用于导出新的不确定性声明的公理。在人工智能领域，概率论主要有两种用途：
  - 概率法则告诉我们 AI 系统如何推理。
  - 用概率和统计从理论上分析 AI 系统的行为。


** 3.1 为什么要使用概率？
   除了那些被定义为真的数学声明，我们很难认定某个命题是千真万确的或者确保某
   件事一定会发生。

   不确定性有三种可能的来源：
   1. 被建模系统内在的随机性。
   2. 不完全观测。
   3. 不完全建模。

      
   在很多情况下，使用一些简单而不确定的规则要比复杂而确定的规则更为实用，即使真
   正的规则是确定的并且我们建模的系统可以足够精确地容纳复杂的规则。



** 3.3 概率分布
   离散型变量的概率分布可以用 *概率质量函数* （PMF) 来描述。

   如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件：
   - P 的定义域必须是 x 所有可能状态的集合。
   - ∀x ∈ x,0 ≤ P(x) ≤ 1 。
   - ∑x∈x P(x) = 1 。我们把这条性质称之为 *归一化的* （normalized）。



   连续型随机变量使用 *概率密度函数* （PDF) 来描述。

   如果一个函数 p 是概率密度函数，必须满足下面这几个条件：
   - p 的定义域必须是 x 所有可能状态的集合。
   - ∀x ∈ x,p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1。
   - ∫p(x)dx = 1 。



** 3.5 条件概率
   *条件概率* 是指某个事件在给定其他事件发生时出现的概率。如给定事件 B，事件 A
   发生的条件概率记为 =P(A|B)= 。

   条件概率可以通过下面的公式计算：
   #+BEGIN_CENTER
   P(A|B) = P(A,B) / P(A)   
   #+END_CENTER
   其中， =P(A,B)= 为事件A和事件B同时发生的概率。



   条件概率具有 *链式法则* （或称为 *乘法法则* ）。
   
   简单的例子如下：
   #+BEGIN_EXAMPLE
   P(a,b,c) = P(a | b,c)P(b,c)
   P(b,c) = P(b | c)P(c)
   P(a,b,c) = P(a | b,c)P(b | c)P(c)
   #+END_EXAMPLE



** 3.7 独立性和条件独立性
   两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并且一个
   因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是 *相互独立的* ：
   #+BEGIN_CENTER
   ∀x ∈ x,y ∈ y,p(x = x,y = y) = p(x = x)p(y = y).
   #+END_CENTER
   
   x⊥y 表示 x 和 y 相互独立。

   在某给定条件下，仍相互独立，则称为 *条件独立的* 。

** 3.8 期望、方差和协方差
   *期望* 是基于概率基础的，是对未知的预期。

   以离散情况为例。
   #+BEGIN_CENTER
   E(X)= ∑ x_i p_i     
   #+END_CENTER

   首先是已知在每一状态 i 下的取值 x_i ，以及概率 p_i 。然后才能推断出期望。而概
   率在大多出情况下是由频数近似而来的。

   期望是线性的，例如，
   #+BEGIN_CENTER
   E_x [αf(x) + βg(x)] = αE_x [f(x)] + βE_x [g(x)], 
   #+END_CENTER



   *方差* 衡量的是当我们对 x 依据它的概率分布进行采样时，随机变量 x 的函数值会呈
   现多大的差异。也就是度量随机变量和其数学期望（即均值）之间的偏离程度。
   #+BEGIN_CENTER
   D(X) = E[(x - E[x])^2] = ∑ p_i (x_i - E[x])^2   
   #+END_CENTER



   *协方差* 在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：
   #+BEGIN_CENTER
   Cov(x, y) = E[(x - E[x])(y - E[y])]
   #+END_CENTER



   随机向量 x ∈ R^n 的协方差矩阵是一个 n × n 的矩阵，并且满足
   #+BEGIN_CENTER
   Cov(x)_i,j = Cov(x_i, x_j)
   #+END_CENTER

** 3.9 常用概率分布
*** 3.9.1 Bernoulli 伯努利分布
    *伯努利分布* 亦称“ *零一分布* ”、“ *两点分布* ”。称随机变量X有伯努利分布，
    参数为 p（0<p<1），如果它分别以概率 p 和 1-p 取 1 和 0 为值。


*** 3.9.3 高斯分布
    *正态分布* 也称为 *高斯分布* 。

    采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布
    的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两
    个原因。

    第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。 *中心极限定理*
    说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可
    以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。

    第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定
    性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

** 3.10 常用函数的有用性质
   *logistic sigmoid 函数* ：
   #+BEGIN_CENTER
   σ(x) = 1 / (1 + exp(−x))   
   #+END_CENTER
   logistic sigmoid 函数通常用来产生 Bernoulli 分布中的参数 p。

   

   *softplus 函数* 
   #+BEGIN_CENTER
   ζ(x) = log(1 + exp(x))
   #+END_CENTER

   softplus 函数可以用来产生正态分布的 β 和 σ 参数，因为它的范围是 (0, ∞)。


** 3.11 贝叶斯规则
   *贝叶斯规则* 实现了已知 P(y|x) 时计算 P(x|y) ：
   #+BEGIN_CENTER
   P(x|y) = P(x)P(y|x) / P(y)
   #+END_CENTER


** 3.13 信息论
   信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。它
   最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消息，例如通过
   无线电传输来通信。在这种情况下，信息论告诉我们如何对消息设计最优编码以及计算
   消息的期望长度，这些消息是使用多种不同编码机制、从特定的概率分布上采样得到的。
   在机器学习中，我们也可以把信息论应用于连续型变量，此时某些消息长度的解释不再
   适用。

   
   在本书中，我们主要使用信息论的一些关键思想来描述概率分布或者量化概率分布之间
   的相似性。

   信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，
   能提供更多的信息。

   我们想要通过这种基本想法来量化信息。特别地，
   - 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没
     有信息量。
   - 较不可能发生的事件具有更高的信息量。
   - 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是
     投掷一次硬币正面朝上的信息量的两倍。


** 3.14 结构化概率模型
   当我们用图来表示这种概率分布的分解，我们把它称为 *结构化概率模型* 或者
   *图模型* 。

   有两种主要的结构化概率模型：有向的和无向的。
